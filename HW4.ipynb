{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sn\n",
    "import statistics as st\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy\n",
    "\n",
    "#linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset from R\n",
    "Auto = sm.datasets.get_rdataset(\"Auto\",\"ISLR\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we split the set of observations into two halves, by selecting a random subset of 196 observations out of the original 392 observations. We refer to these observations as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Auto['horsepower'].values.reshape(-1,1)\n",
    "y = Auto['mpg'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer dataframe into list\n",
    "Auto_list = Auto.values.tolist()\n",
    "#split in training set\n",
    "train = random.sample(Auto_list, 196)\n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training set with n=196 and testing set with n=196\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the subset to fit a linear regression using only the observations corresponding to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "lr = LinearRegression()  \n",
    "lr.fit(X_train, y_train) #training the algorithm\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the response for all 392 observations, and we calculate the MSE of the 196 observations in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 23.616617069669882\n"
     ]
    }
   ],
   "source": [
    "#MSE\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the estimated test MSE for the linear regression fit is 23.617."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find the MSE for quadratic and cubic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 18.763031346897744\n"
     ]
    }
   ],
   "source": [
    "#quadratic regression\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "X = polynomial_features.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "lr.fit(X_train, y_train) #training the algorithm\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 18.194589356943503\n"
     ]
    }
   ],
   "source": [
    "#cubic regression\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "X = polynomial_features.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "lr.fit(X_train, y_train) #training the algorithm\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The error rates are 18.76 and 18.19, respectively. If we choose a different training set instead, then we will obtain somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are consistent with our previous findings: a model that predicts mpg using a **quadratic function** of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence in favor of a model that uses a cubic function of horsepower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Leave-One-Out Cross-Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: Intercept, Beta(s) 39.93586102117047 [-0.15784473]\n",
      "Coefficients: Intercept, Beta(s) 56.90009970211297 [-0.46618963  0.00123054]\n",
      "Coefficients: Intercept, Beta(s) 60.68478490666118 [-5.68850128e-01  2.07901126e-03 -2.14662591e-06]\n",
      "Coefficients: Intercept, Beta(s) 47.567677230017296 [-7.66721350e-02 -4.34464058e-03  3.24511548e-05 -6.53036297e-08]\n",
      "Coefficients: Intercept, Beta(s) -32.23055925051304 [ 3.70010897e+00 -7.14240776e-02  5.93108571e-04 -2.28107628e-06\n",
      "  3.32955307e-09]\n",
      "\n",
      "The estimated test MSEs =  [24.23151352 19.24821312 19.33498406 19.42443031 19.0332128 ]\n"
     ]
    }
   ],
   "source": [
    "X = Auto.horsepower; \n",
    "X = X[:,np.newaxis];\n",
    "y = Auto.mpg\n",
    "orders = np.arange(1,6)\n",
    "mse_est = np.array([])\n",
    "for index, order in enumerate(orders):\n",
    "    poly = PolynomialFeatures(degree=order, interaction_only=False, include_bias=False)\n",
    "    regress = LinearRegression()\n",
    "    regress.fit(poly.fit_transform(X), y)\n",
    "    print('Coefficients: Intercept, Beta(s)', regress.intercept_, regress.coef_)\n",
    "    mse_est = np.append(mse_est, -np.mean(cross_val_score(regress, poly.fit_transform(X), y,\n",
    "                                                                          cv=len(X), scoring='neg_mean_squared_error')))\n",
    "print('\\nThe estimated test MSEs = ', mse_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this procedure for increasingly complex polynomial fits. To automate the process, we initiate a loop which iteratively fits polynomial regressions for polynomials of order i = 1 to i = 5, computes the associated cross-validation error, and stores it in the ith element of the vector cv.error. We begin by initializing the vector. From the result above can we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: Intercept, Beta(s) 39.93586102117047 [-0.15784473]\n",
      "Coefficients: Intercept, Beta(s) 56.90009970211297 [-0.46618963  0.00123054]\n",
      "Coefficients: Intercept, Beta(s) 60.68478490666118 [-5.68850128e-01  2.07901126e-03 -2.14662591e-06]\n",
      "Coefficients: Intercept, Beta(s) 47.567677230017296 [-7.66721350e-02 -4.34464058e-03  3.24511548e-05 -6.53036297e-08]\n",
      "Coefficients: Intercept, Beta(s) -32.23055925051304 [ 3.70010897e+00 -7.14240776e-02  5.93108571e-04 -2.28107628e-06\n",
      "  3.32955307e-09]\n",
      "Coefficients: Intercept, Beta(s) -162.14413961110918 [ 1.12381147e+01 -2.43636568e-01  2.58014076e-03 -1.45299453e-05\n",
      "  4.17283090e-08 -4.80321240e-11]\n",
      "Coefficients: Intercept, Beta(s) 14.488249578731345 [ 1.26465278e-03  4.69956315e-02 -1.39312752e-03  1.65893517e-05\n",
      " -9.84641105e-08  2.89696707e-10 -3.36931900e-13]\n",
      "Coefficients: Intercept, Beta(s) 37.41710536512173 [ 5.07503727e-08  3.13402515e-06  9.50797255e-05 -4.46285569e-06\n",
      "  6.36102185e-08 -4.12421843e-10  1.27136133e-12 -1.51546201e-15]\n",
      "Coefficients: Intercept, Beta(s) 40.63798394696036 [-1.73110788e-11  2.07682597e-08 -7.47703096e-08 -2.04670469e-06\n",
      "  4.13723020e-08 -3.45661801e-10  1.46810621e-12 -3.14443955e-15\n",
      "  2.71074219e-18]\n",
      "\n",
      "The estimated test MSEs =  [27.43993365 21.23584006 21.33660618 21.35388698 20.90564077 20.7801154\n",
      " 20.96809438 21.07823479 21.03760238]\n"
     ]
    }
   ],
   "source": [
    "X = Auto.horsepower; \n",
    "X = X[:,np.newaxis];\n",
    "y = Auto.mpg\n",
    "orders = np.arange(1,10)\n",
    "mse_est = np.array([])\n",
    "for index, order in enumerate(orders):\n",
    "    poly = PolynomialFeatures(degree=order, interaction_only=False, include_bias=False)\n",
    "    regress = LinearRegression()\n",
    "    regress.fit(poly.fit_transform(X), y)\n",
    "    print('Coefficients: Intercept, Beta(s)', regress.intercept_, regress.coef_)\n",
    "    mse_est = np.append(mse_est, -np.mean(cross_val_score(regress, poly.fit_transform(X), y,\n",
    "                                                                          cv=10, scoring='neg_mean_squared_error')))\n",
    "print('\\nThe estimated test MSEs = ', mse_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Accuracy of a Statistic of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function alpha.fn\n",
    "def alpha_fn(data,index):\n",
    "    X = data.X[index]\n",
    "    Y = data.Y[index]\n",
    "    return((st.variance(Y)-np.cov(X,Y)[0][1])/(st.variance(X)+st.variance(Y)-2*np.cov(X,Y)[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset from R\n",
    "Portfolio = sm.datasets.get_rdataset(\"Portfolio\",\"ISLR\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns an estimate for α based on applying (5.7) to the observations indexed by the argument index. For instance, the following command tells Python to estimate α using all 100 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command randomly select 100 observations from the range 1 to 100, with replacement. This is equivalent to constructing a new bootstrap data set and recomputing αˆ based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5184949611013724"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, np.random.choice(100,100, replace = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for α, and computing the resulting standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boot function\n",
    "def boot(data, statsfunc, num_bootstrap_samples=1000, sample_size=100):\n",
    "    stat_samples = []\n",
    "    for sample in range(num_bootstrap_samples):\n",
    "        indices = np.random.choice(data.index, sample_size, replace=True)\n",
    "        stat_samples.append(statsfunc(data, indices))\n",
    "    se_estimate = scipy.std(stat_samples, axis=0)\n",
    "    print('\\nBootstrapped Std. Error(s) =', se_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapped Std. Error(s) = 0.09100260584187886\n"
     ]
    }
   ],
   "source": [
    "boot(Portfolio, alpha_fn, 1000, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output shows that using the original data, the bootstrap estimate for SE(αˆ) is 0.091."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Accuracy of a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.93586102, -0.15784473])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boot_fn(data, index):\n",
    "    X = data.horsepower[index].values\n",
    "    X = sm.add_constant(X)\n",
    "    y = data.mpg[index].values\n",
    "    lm_fit = sm.OLS(y, X).fit()\n",
    "    return lm_fit.params\n",
    "\n",
    "boot_fn(Auto, Auto.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use another method to get the similar result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.65847877, -0.15589835])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1); \n",
    "indices = np.random.choice(Auto.index, len(Auto), replace=True)\n",
    "\n",
    "boot_fn(Auto, indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapped Std. Error(s) = [0.82521528 0.00713533]\n"
     ]
    }
   ],
   "source": [
    "boot(Auto, boot_fn, num_bootstrap_samples=1000, sample_size=len(Auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the bootstrap estimate for SE(β0) is 0.825, and that the bootstrap estimate for SE(β1) is 0.007. As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. The summary of result is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   599.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>7.03e-81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:34:06</td>     <th>  Log-Likelihood:    </th> <td> -1178.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2361.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   390</td>      <th>  BIC:               </th> <td>   2369.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>   39.9359</td> <td>    0.717</td> <td>   55.660</td> <td> 0.000</td> <td>   38.525</td> <td>   41.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th> <td>   -0.1578</td> <td>    0.006</td> <td>  -24.489</td> <td> 0.000</td> <td>   -0.171</td> <td>   -0.145</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>16.432</td> <th>  Durbin-Watson:     </th> <td>   0.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  17.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.492</td> <th>  Prob(JB):          </th> <td>0.000175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.299</td> <th>  Cond. No.          </th> <td>    322.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.606\n",
       "Model:                            OLS   Adj. R-squared:                  0.605\n",
       "Method:                 Least Squares   F-statistic:                     599.7\n",
       "Date:                Fri, 28 Feb 2020   Prob (F-statistic):           7.03e-81\n",
       "Time:                        23:34:06   Log-Likelihood:                -1178.7\n",
       "No. Observations:                 392   AIC:                             2361.\n",
       "Df Residuals:                     390   BIC:                             2369.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     39.9359      0.717     55.660      0.000      38.525      41.347\n",
       "horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n",
       "==============================================================================\n",
       "Omnibus:                       16.432   Durbin-Watson:                   0.920\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\n",
       "Skew:                           0.492   Prob(JB):                     0.000175\n",
       "Kurtosis:                       3.299   Cond. No.                         322.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('mpg~horsepower', Auto).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Although the formula for the standard errors do not rely on the linear model being correct, the estimate for σ2 does. There is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will σˆ2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of SE(β0), SE(β1) and SE(β2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapped Std. Error(s) = [2.09222532e+00 3.32006462e-02 1.19440327e-04]\n"
     ]
    }
   ],
   "source": [
    "def boot_fn(df, indices):\n",
    "    X = np.column_stack([df.horsepower[indices].values, df.horsepower[indices].values**2])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df.mpg[indices].values\n",
    "    lm_fit = sm.OLS(y, X).fit()\n",
    "    return lm_fit.params\n",
    "boot(Auto, boot_fn, num_bootstrap_samples=1000, sample_size=len(Auto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   428.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>5.40e-99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:38:36</td>     <th>  Log-Likelihood:    </th> <td> -1133.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2272.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   389</td>      <th>  BIC:               </th> <td>   2284.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>               <td>   56.9001</td> <td>    1.800</td> <td>   31.604</td> <td> 0.000</td> <td>   53.360</td> <td>   60.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th>              <td>   -0.4662</td> <td>    0.031</td> <td>  -14.978</td> <td> 0.000</td> <td>   -0.527</td> <td>   -0.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(horsepower, 2)</th> <td>    0.0012</td> <td>    0.000</td> <td>   10.080</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>16.158</td> <th>  Durbin-Watson:     </th> <td>   1.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  30.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.218</td> <th>  Prob(JB):          </th> <td>2.20e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.299</td> <th>  Cond. No.          </th> <td>1.29e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.29e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.688\n",
       "Model:                            OLS   Adj. R-squared:                  0.686\n",
       "Method:                 Least Squares   F-statistic:                     428.0\n",
       "Date:                Fri, 28 Feb 2020   Prob (F-statistic):           5.40e-99\n",
       "Time:                        23:38:36   Log-Likelihood:                -1133.2\n",
       "No. Observations:                 392   AIC:                             2272.\n",
       "Df Residuals:                     389   BIC:                             2284.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===========================================================================================\n",
       "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------\n",
       "Intercept                  56.9001      1.800     31.604      0.000      53.360      60.440\n",
       "horsepower                 -0.4662      0.031    -14.978      0.000      -0.527      -0.405\n",
       "np.power(horsepower, 2)     0.0012      0.000     10.080      0.000       0.001       0.001\n",
       "==============================================================================\n",
       "Omnibus:                       16.158   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               30.662\n",
       "Skew:                           0.218   Prob(JB):                     2.20e-07\n",
       "Kurtosis:                       4.299   Cond. No.                     1.29e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.29e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols('mpg ~ horsepower + np.power(horsepower, 2)', Auto).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>['default[No]', 'default[Yes]']</td> <th>  No. Observations:  </th>  <td> 10000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                         <td>GLM</td>               <th>  Df Residuals:      </th>  <td>  9997</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>               <td>Binomial</td>             <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                <td>logit</td>              <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                       <td>IRLS</td>               <th>  Log-Likelihood:    </th> <td> -789.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                   <td>Sat, 29 Feb 2020</td>         <th>  Deviance:          </th> <td>  1579.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                       <td>11:27:37</td>             <th>  Pearson chi2:      </th> <td>6.95e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                 <td>9</td>                <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>            <td>nonrobust</td>            <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   11.5405</td> <td>    0.435</td> <td>   26.544</td> <td> 0.000</td> <td>   10.688</td> <td>   12.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td>-2.081e-05</td> <td> 4.99e-06</td> <td>   -4.174</td> <td> 0.000</td> <td>-3.06e-05</td> <td> -1.1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>   -0.0056</td> <td>    0.000</td> <td>  -24.835</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                        Generalized Linear Model Regression Results                        \n",
       "===========================================================================================\n",
       "Dep. Variable:     ['default[No]', 'default[Yes]']   No. Observations:                10000\n",
       "Model:                                         GLM   Df Residuals:                     9997\n",
       "Model Family:                             Binomial   Df Model:                            2\n",
       "Link Function:                               logit   Scale:                          1.0000\n",
       "Method:                                       IRLS   Log-Likelihood:                -789.48\n",
       "Date:                             Sat, 29 Feb 2020   Deviance:                       1579.0\n",
       "Time:                                     11:27:37   Pearson chi2:                 6.95e+03\n",
       "No. Iterations:                                  9                                         \n",
       "Covariance Type:                         nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     11.5405      0.435     26.544      0.000      10.688      12.393\n",
       "income     -2.081e-05   4.99e-06     -4.174      0.000   -3.06e-05    -1.1e-05\n",
       "balance       -0.0056      0.000    -24.835      0.000      -0.006      -0.005\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.glm('default ~ income + balance', Default, family=sm.families.Binomial()).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated standard errors for the coefficients associated with income and balance is 4.99e-06 and 0, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(default):\n",
    "    model = smf.glm('default ~ income + balance', Default, family=sm.families.Binomial()).fit()\n",
    "    coef_income = model.params[1]\n",
    "    coef_balance = model.params[2]\n",
    "    return [coef_income, coef_balance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.080897552898698e-05, -0.005647102950316488]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_fn(Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boot function\n",
    "def boot(data, statsfunc, num_bootstrap_samples=1000, sample_size=100):\n",
    "    stat_samples = []\n",
    "    for sample in range(num_bootstrap_samples):\n",
    "        stat_samples.append(statsfunc(data))\n",
    "    se_estimate = scipy.std(stat_samples, axis=0)\n",
    "    print('\\nBootstrapped Std. Error(s) =', se_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapped Std. Error(s) = [6.77626358e-20 3.20923843e-17]\n"
     ]
    }
   ],
   "source": [
    "boot(Default, boot_fn, 1000, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in part b is larger than that of part c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset from R\n",
    "Weekly = sm.datasets.get_rdataset(\"Weekly\",\"ISLR\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5547695090975024 [-0.00945186  0.01462384] 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "#transer Direction into dummy variable\n",
    "Weekly['Direction_Up'] = (Weekly['Direction'] == 'Up').astype(int)\n",
    "\n",
    "X = Weekly[['Lag1', 'Lag2']]\n",
    "y = Weekly.Direction_Up\n",
    "\n",
    "lr.fit(X,y)\n",
    "print(lr.intercept_, lr.coef_, lr.predict(X).mean())  # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5552615405384282 [-0.00937857  0.01476349] 0.5560797216067374\n"
     ]
    }
   ],
   "source": [
    "#remove first observation\n",
    "lr.fit(X.iloc[1:],y.iloc[1:])\n",
    "\n",
    "print(lr.intercept_, lr.coef_, lr.predict(X).mean())  # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56956525]\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict([X.iloc[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the first observation = 0.57 > 0.5, so we classified it as \"Up\". However, the true value is \"Down\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(len(X))\n",
    "predicts = np.zeros(len(X))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    lr.fit(X.drop([i]),y.drop([i]))\n",
    "    if lr.predict([X.iloc[i]]) > 0.5:\n",
    "        predicts[i] = 1\n",
    "    else:\n",
    "        predicts[i] = 0\n",
    "    if predicts[i] != y[i]:\n",
    "        errors[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44811753902662993"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate for the test error is 0.448, which means the model is not good at predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset from R\n",
    "Boston = sm.datasets.get_rdataset(\"Boston\",\"MASS\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110698"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#population mean of medv\n",
    "mu = Boston.medv.mean()\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4088611474975351"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE_medv = Boston.medv.std()/np.sqrt(len(Boston))\n",
    "SE_medv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39920199850137605"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SE of mean using bootstrap\n",
    "means = [Boston.medv.sample(n = len(Boston), replace=True).mean() for _ in range(1000)]\n",
    "se = np.std(means)\n",
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated SE in part c is smaller than that in part b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.734402327107947 23.33121032111345\n"
     ]
    }
   ],
   "source": [
    "print(mu-2*se, mu+2*se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.2"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_med = Boston.medv.median()\n",
    "mu_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38953810789189774"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SE of medain using bootstrap\n",
    "median = [Boston.medv.sample(n = len(Boston), replace=True).median() for _ in range(1000)]\n",
    "se_med = np.std(median)\n",
    "se_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the standard error of mean, the SE of median is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.75"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston.medv.quantile(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5015126493918174"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SE of quantile 0.1 using bootstrap\n",
    "q = [Boston.medv.sample(n = len(Boston), replace=True).quantile(0.1) for _ in range(1000)]\n",
    "se_qtl = np.std(q)\n",
    "se_qtl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SE is 0.5, which is larger than that of part c and part f."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
