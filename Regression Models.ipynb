{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Subset selection in python\n",
    "# \n",
    "# \n",
    "# This notebook explores common methods for performing subset selection on a regression model, namely\n",
    "# \n",
    "#     Best subset selection\n",
    "#     Forward stepwise selection\n",
    "#     Criteria for choosing the optimal model\n",
    "#         Cp, AIC, BIC, R2adj\n",
    "# \n",
    "# ## [Lab 2: Ridge Regression](#Ridge-Regression)\n",
    "# [Hitters Data](#Read-training-and-Test-data)\n",
    "# - [The Lasso](#The-LASSO)\n",
    "# - [Lab 3: Principal Components](#Principal-Components)\n",
    "# - [Lab 3: Partial Least Squares](#Partial-Least-Squares)\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\AST\\\\OneDrive - Southern Methodist University\\\\Documents\\\\Documents\\\\SMU\\\\Machine Learning\\\\ISL Data\\\\IntroStatLearn-master\\\\data\\\\\")\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# # Credit Data\n",
    "# The Credit data set displayed in Figure records balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating). Each panel of Figure 3.6 is a scatterplot for a pair of variables whose identities are given by the corresponding row and column labels. For example, the scatterplot directly to the right of the word “Balance” depicts balance versus age, while the plot directly to the right of “Age” corresponds to age versus cards. In addition to these quantitative variables, we also have four qualitative variables: gender, student (student status), status (marital status), and ethnicity (Caucasian, African American or Asian).\n",
    "# [Statsmodel linear regression ](#first)\n",
    "\n",
    "# In[217]:\n",
    "\n",
    "\n",
    "credit = pd.read_csv('Credit.csv', usecols=list(range(1,12)))\n",
    "credit.head(3)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[218]:\n",
    "\n",
    "\n",
    "sns.pairplot(credit[['Balance','Age','Cards','Education','Income','Limit','Rating']]);\n",
    "\n",
    "\n",
    "# ## Statsmodel linear regression \n",
    "# <a id='first'></a>\n",
    "# \n",
    "# Least squares coefficient estimates associated with the regression of balance onto ethnicity in the Credit data set. The linear model is given in (3.30). That is, ethnicity is encoded via two dummy variables\n",
    "# \n",
    "\n",
    "# In[221]:\n",
    "\n",
    "\n",
    "#Setting up the X and Y variables, adding constant term for intercept\n",
    "Y = credit.Balance\n",
    "X = credit[['Ethnicity_Asian','Ethnicity_Caucasian']]\n",
    "X = sm.add_constant(X)\n",
    "X.head()\n",
    "\n",
    "\n",
    "# In[222]:\n",
    "\n",
    "\n",
    "# OLS Table\n",
    "model_1 = sm.OLS(Y, X)\n",
    "result_1 = model_1.fit()\n",
    "result_1.summary()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "credit = pd.read_csv('Credit.csv', usecols=list(range(1,12))\n",
    "\n",
    "\n",
    "# In[220]:\n",
    "\n",
    "\n",
    "credit = pd.get_dummies(credit, columns = ['Gender', 'Student','Married','Ethnicity'],drop_first = True)\n",
    "credit.head(5)\n",
    "\n",
    "\n",
    "# ### Define function for fitting linear regression (Sklearn)¶\n",
    "\n",
    "# In[223]:\n",
    "\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = linear_model.LinearRegression(fit_intercept = True)\n",
    "    model_k.fit(X,Y)\n",
    "    RSS = mean_squared_error(Y,model_k.predict(X)) * len(Y)\n",
    "    R_squared = model_k.score(X,Y)\n",
    "    return RSS, R_squared\n",
    "\n",
    "\n",
    "# ## Implementing Best subset selection (using itertools.combinations)\n",
    "\n",
    "# In[224]:\n",
    "\n",
    "\n",
    "#Importing tqdm for the progress bar\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "#Initialization variables\n",
    "Y = credit.Balance\n",
    "X = credit.drop(columns = 'Balance', axis = 1)\n",
    "k = 11\n",
    "RSS_list, R_squared_list, feature_list = [],[], []\n",
    "numb_features = []\n",
    "\n",
    "#Looping over k = 1 to k = 11 features in X\n",
    "for k in tnrange(1,len(X.columns) + 1, desc = 'Loop...'):\n",
    "\n",
    "    #Looping over all possible combinations: from 11 choose k\n",
    "    for combo in itertools.combinations(X.columns,k):\n",
    "        tmp_result = fit_linear_reg(X[list(combo)],Y)   #Store temp result \n",
    "        RSS_list.append(tmp_result[0])                  #Append lists\n",
    "        R_squared_list.append(tmp_result[1])\n",
    "        feature_list.append(combo)\n",
    "        numb_features.append(len(combo))   \n",
    "\n",
    "#Store in DataFrame\n",
    "df = pd.DataFrame({'numb_features': numb_features,'RSS': RSS_list, 'R_squared':R_squared_list,'features':feature_list})\n",
    "\n",
    "\n",
    "# ### Finding the best subsets for each number of features\n",
    "# \n",
    "# #### Using the smallest RSS value, or the largest R_squared value\n",
    "# \n",
    "\n",
    "# In[225]:\n",
    "\n",
    "\n",
    "df_min = df[df.groupby('numb_features')['RSS'].transform(min) == df['RSS']]\n",
    "df_max = df[df.groupby('numb_features')['R_squared'].transform(max) == df['R_squared']]\n",
    "display(df_min.head(3))\n",
    "display(df_max.head(3))\n",
    "\n",
    "\n",
    "# ### Adding columns to the dataframe with RSS and R squared values of the best subset\n",
    "\n",
    "# In[226]:\n",
    "\n",
    "\n",
    "df['min_RSS'] = df.groupby('numb_features')['RSS'].transform(min)\n",
    "df['max_R_squared'] = df.groupby('numb_features')['R_squared'].transform(max)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# ## Plotting the best subset selection process\n",
    "\n",
    "# In[227]:\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (16,6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "ax.scatter(df.numb_features,df.RSS, alpha = .2, color = 'darkblue' )\n",
    "ax.set_xlabel('# Features')\n",
    "ax.set_ylabel('RSS')\n",
    "ax.set_title('RSS - Best subset selection')\n",
    "ax.plot(df.numb_features,df.min_RSS,color = 'r', label = 'Best subset')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.scatter(df.numb_features,df.R_squared, alpha = .2, color = 'darkblue' )\n",
    "ax.plot(df.numb_features,df.max_R_squared,color = 'r', label = 'Best subset')\n",
    "ax.set_xlabel('# Features')\n",
    "ax.set_ylabel('R squared')\n",
    "ax.set_title('R_squared - Best subset selection')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## Forward stepwise selection\n",
    "\n",
    "# In[228]:\n",
    "\n",
    "\n",
    "#Initialization variables\n",
    "Y = credit.Balance\n",
    "X = credit.drop(columns = 'Balance', axis = 1)\n",
    "k = 11\n",
    "\n",
    "remaining_features = list(X.columns.values)\n",
    "features = []\n",
    "RSS_list, R_squared_list = [np.inf], [np.inf] #Due to 1 indexing of the loop...\n",
    "features_list = dict()\n",
    "\n",
    "for i in range(1,k+1):\n",
    "    best_RSS = np.inf\n",
    "    \n",
    "    for combo in itertools.combinations(remaining_features,1):\n",
    "\n",
    "            RSS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result \n",
    "\n",
    "            if RSS[0] < best_RSS:\n",
    "                best_RSS = RSS[0]\n",
    "                best_R_squared = RSS[1] \n",
    "                best_feature = combo[0]\n",
    "\n",
    "    #Updating variables for next loop\n",
    "    features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    \n",
    "    #Saving values for plotting\n",
    "    RSS_list.append(best_RSS)\n",
    "    R_squared_list.append(best_R_squared)\n",
    "    features_list[i] = features.copy()\n",
    "\n",
    "\n",
    "# In[229]:\n",
    "\n",
    "\n",
    "print('Forward stepwise subset selection')\n",
    "print('Number of features |', 'Features |', 'RSS')\n",
    "display([(i,features_list[i], round(RSS_list[i])) for i in range(1,5)])\n",
    "\n",
    "\n",
    "# ## Comparing models: AIC, BIC, Mallows'CP\n",
    "\n",
    "# In[231]:\n",
    "\n",
    "\n",
    "df1 = pd.concat([pd.DataFrame({'features':features_list}),pd.DataFrame({'RSS':RSS_list, 'R_squared': R_squared_list})], axis=1, join='inner')\n",
    "df1['numb_features'] = df1.index\n",
    "\n",
    "\n",
    "# ### Computing the C_p, AIC, BIC and R-square adjusted\n",
    "\n",
    "# In[232]:\n",
    "\n",
    "\n",
    "#Initializing useful variables\n",
    "m = len(Y)\n",
    "p = 11\n",
    "hat_sigma_squared = (1/(m - p -1)) * min(df1['RSS'])\n",
    "\n",
    "#Computing\n",
    "df1['C_p'] = (1/m) * (df1['RSS'] + 2 * df1['numb_features'] * hat_sigma_squared )\n",
    "df1['AIC'] = (1/(m*hat_sigma_squared)) * (df1['RSS'] + 2 * df1['numb_features'] * hat_sigma_squared )\n",
    "df1['BIC'] = (1/(m*hat_sigma_squared)) * (df1['RSS'] +  np.log(m) * df1['numb_features'] * hat_sigma_squared )\n",
    "df1['R_squared_adj'] = 1 - ( (1 - df1['R_squared'])*(m-1)/(m-df1['numb_features'] -1))\n",
    "df1\n",
    "\n",
    "\n",
    "# In[233]:\n",
    "\n",
    "\n",
    "df1['R_squared_adj'].idxmax()\n",
    "df1['R_squared_adj'].max()\n",
    "\n",
    "\n",
    "# ### Plotting the computed values as a function of number of features\n",
    "\n",
    "# In[234]:\n",
    "\n",
    "\n",
    "variables = ['C_p', 'AIC','BIC','R_squared_adj']\n",
    "fig = plt.figure(figsize = (18,6))\n",
    "\n",
    "for i,v in enumerate(variables):\n",
    "    ax = fig.add_subplot(1, 4, i+1)\n",
    "    ax.plot(df1['numb_features'],df1[v], color = 'lightblue')\n",
    "    ax.scatter(df1['numb_features'],df1[v], color = 'darkblue')\n",
    "    if v == 'R_squared_adj':\n",
    "        ax.plot(df1[v].idxmax(),df1[v].max(), marker = 'x', markersize = 20)\n",
    "    else:\n",
    "        ax.plot(df1[v].idxmin(),df1[v].min(), marker = 'x', markersize = 20)\n",
    "    ax.set_xlabel('Number of predictors')\n",
    "    ax.set_ylabel(v)\n",
    "\n",
    "fig.suptitle('Subset selection using C_p, AIC, BIC, Adjusted R2', fontsize = 16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Ridge Regression\n",
    "\n",
    "# \n",
    "\n",
    "# ## Read the Hitters data\n",
    "# \n",
    "# Description\n",
    "# \n",
    "# Baseball Data\n",
    "# Description\n",
    "# \n",
    "# Major League Baseball Data from the 1986 and 1987 seasons.\n",
    "# Usage\n",
    "# \n",
    "# Hitters\n",
    "# \n",
    "# Format\n",
    "# \n",
    "# A data frame with 322 observations of major league players on the following 20 variables.\n",
    "# \n",
    "# AtBat\n",
    "# \n",
    "#     Number of times at bat in 1986\n",
    "# Hits\n",
    "# \n",
    "#     Number of hits in 1986\n",
    "# HmRun\n",
    "# \n",
    "#     Number of home runs in 1986\n",
    "# Runs\n",
    "# \n",
    "#     Number of runs in 1986\n",
    "# RBI\n",
    "# \n",
    "#     Number of runs batted in in 1986\n",
    "# Walks\n",
    "# \n",
    "#     Number of walks in 1986\n",
    "# Years\n",
    "# \n",
    "#     Number of years in the major leagues\n",
    "# CAtBat\n",
    "# \n",
    "#     Number of times at bat during his career\n",
    "# CHits\n",
    "# \n",
    "#     Number of hits during his career\n",
    "# CHmRun\n",
    "# \n",
    "#     Number of home runs during his career\n",
    "# CRuns\n",
    "# \n",
    "#     Number of runs during his career\n",
    "# CRBI\n",
    "# \n",
    "#     Number of runs batted in during his career\n",
    "# CWalks\n",
    "# \n",
    "#     Number of walks during his career\n",
    "# League\n",
    "# \n",
    "#     A factor with levels A and N indicating player's league at the end of 1986\n",
    "# Division\n",
    "# \n",
    "#     A factor with levels E and W indicating player's division at the end of 1986\n",
    "# PutOuts\n",
    "# \n",
    "#     Number of put outs in 1986\n",
    "# Assists\n",
    "# \n",
    "#     Number of assists in 1986\n",
    "# Errors\n",
    "# \n",
    "#     Number of errors in 1986\n",
    "# Salary\n",
    "# \n",
    "#     1987 annual salary on opening day in thousands of dollars\n",
    "# NewLeague\n",
    "# \n",
    "#     A factor with levels A and N indicating player's league at the beginning of 1987\n",
    "# \n",
    "# Source\n",
    "# \n",
    "# This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York. \n",
    "# \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "hitters = pd.read_csv('Hitters.csv', index_col=0).dropna()\n",
    "hitters.index.name = 'Player'\n",
    "hitters.info()\n",
    "\n",
    "\n",
    "# In[235]:\n",
    "\n",
    "\n",
    "hitters.head()\n",
    "\n",
    "\n",
    "# In[236]:\n",
    "\n",
    "\n",
    "dummies = pd.get_dummies(hitters[['League', 'Division', 'NewLeague']])\n",
    "dummies.info()\n",
    "print(dummies.head())\n",
    "\n",
    "\n",
    "# #### Define X and y matrices:\n",
    "#     Drop the column with the independent variable (Salary), and qualitative columns \n",
    "# \n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "y = hitters.Salary\n",
    "X_ = hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "X.info()\n",
    "\n",
    "\n",
    "# X.head(5)\n",
    "\n",
    "# ### Read training and Test data\n",
    "\n",
    "# ### SKIP THIS\n",
    "# X_train = pd.read_csv('Hitters_X_train.csv', index_col=0)\n",
    "# y_train = pd.read_csv('Hitters_y_train.csv', index_col=0)\n",
    "# X_test = pd.read_csv('Hitters_X_test.csv', index_col=0)\n",
    "# y_test = pd.read_csv('Hitters_y_test.csv', index_col=0)\n",
    "\n",
    "# #### Generate Alphas to use in Ridge Regression\n",
    "#     \n",
    "\n",
    "# In[239]:\n",
    "\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 # np.linspace(1000,-1000,100)\n",
    "alphas\n",
    "\n",
    "\n",
    "# In[240]:\n",
    "\n",
    "\n",
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)\n",
    "\n",
    "\n",
    "# Output above means that there is a 19x100 matrix, 100 models each with 19 predictors\n",
    "\n",
    "# Let's cerate the plot for when features come into the model\n",
    "\n",
    "# In[241]:\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "\n",
    "\n",
    "# #### Split data into training and test sets \n",
    "# Use train_test_split from sklearn\n",
    "# \n",
    "\n",
    "# In[242]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "\n",
    "# # Let's use alpha=10, then fit  ridge regression on the training data to predict test MSE\n",
    "\n",
    "# In[244]:\n",
    "\n",
    "\n",
    "ridge2 = Ridge(alpha = 5, normalize = True) \n",
    "ridge2.fit(X_train, y_train)            \n",
    "pred2 = ridge2.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred2))          # Calculate the test MSE\n",
    "\n",
    "\n",
    "# #### MSE_test=121263.26 for alpha = 10 \n",
    "# Now let's see for alpha= $10^{5}$, look for shrinkage hence more biased:\n",
    "\n",
    "# In[248]:\n",
    "\n",
    "\n",
    "ridge3 = Ridge(alpha = 10**5, normalize = True)\n",
    "ridge3.fit(X_train, y_train)             \n",
    "pred3 = ridge3.predict(X_test)           \n",
    "print(pd.Series(ridge3.coef_, index = X.columns)) \n",
    "print(mean_squared_error(y_test, pred3))          \n",
    "\n",
    "\n",
    "# Above number indicates a big shrinkage, very small coefficients. \n",
    "# ##### Q: Is the model more or or less biased in this case?\n",
    "# Compare this result to alpha=10 MSE\n",
    "# ### Remember OLS estimate is alpha=0\n",
    "\n",
    "# In[246]:\n",
    "\n",
    "\n",
    "ridge2 = Ridge(alpha = 0, normalize = True)\n",
    "ridge2.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred = ridge2.predict(X_test)            # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred))           # Calculate the test MSE\n",
    "\n",
    "\n",
    "# Is this an improvement over alpha=10 value?\n",
    "# If not reduce the alpha.\n",
    "\n",
    "# ### Let's use cross-validation to choose the tuning parameter alpha. \n",
    "# RidgeCV() is the cross-validated ridge regression function. For the value of alpha from running this, we can check the test MSE\n",
    "\n",
    "# In[247]:\n",
    "\n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_\n",
    "\n",
    "\n",
    "# Once we have the value of alpha, we can calculate new test MSE\n",
    "\n",
    "# In[249]:\n",
    "\n",
    "\n",
    "ridge4 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge4.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge4.predict(X_test))\n",
    "\n",
    "\n",
    "# ### Ridge Regression on full data set with the chosen alpha\n",
    "\n",
    "# In[250]:\n",
    "\n",
    "\n",
    "ridge4.fit(X, y)\n",
    "pd.Series(ridge4.coef_, index = X.columns)\n",
    "mean_squared_error(y, ridge4.predict(X))\n",
    "\n",
    "\n",
    "# In[251]:\n",
    "\n",
    "\n",
    "ridge4.coef_\n",
    "\n",
    "\n",
    "# ## The LASSO\n",
    "# Check to see if the lasso gives a better model (different norm space), i.e.,  a more accurate or a more interpretable model than ridge regression. \n",
    "# \n",
    "\n",
    "# In[158]:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# In[252]:\n",
    "\n",
    "\n",
    "lasso = Lasso(max_iter = 10000, normalize = True) # can change the max iteration x10 or 100\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train), y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "\n",
    "\n",
    "# Q: What's the above graph telling us?  \n",
    "#     A:some coefficients are zero\n",
    "# Now let's look at MSE_test to see if it is any better!\n",
    "\n",
    "# In[253]:\n",
    "\n",
    "\n",
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, lasso.predict(X_test))\n",
    "\n",
    "\n",
    "# Q: Would you use the Lasso in this case?\n",
    "#     A: Much smaller number of features\n",
    "\n",
    "# In[254]:\n",
    "\n",
    "\n",
    "pd.Series(lasso.coef_, index=X.columns)\n",
    "\n",
    "\n",
    "# ## Principal Components \n",
    "# Use sklearn module and PCA() function, primarily. \n",
    "# To continue from above we will use PCA to predict Salary. \n",
    "\n",
    "# check to see if there are any missing values in the data\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "hitters = pd.read_csv('Hitters.csv', index_col=0).dropna()\n",
    "hitters.index.name = 'Player'\n",
    "dummies = pd.get_dummies(hitters[['League', 'Division', 'NewLeague']])\n",
    "#dummies.info()\n",
    "\n",
    "y = hitters.Salary\n",
    "X_ = hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "\n",
    "\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "X.info()\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "#Principal Components\n",
    "pd.DataFrame(pca.components_.T).loc[:,:]\n",
    "\n",
    "\n",
    "# print(X_reduced.shape)\n",
    "# pd.DataFrame(X_reduced).loc[:4,:]\n",
    "\n",
    "# 10-fold cross-validation to see how it influences the MSE:\n",
    "# \n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "n = len(X_reduced)\n",
    "kf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);\n",
    "\n",
    "\n",
    "# We see that the smallest cross-validation error occurs when M=18 components are used. This is barely fewer than M=19\n",
    "# \n",
    "# , which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.\n",
    "# \n",
    "# We'll do a little math to get the amount of variance explained by adding each consecutive principal component:\n",
    "# \n",
    "\n",
    "# ### Variances explained by PCA\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "\n",
    "# setting M=1 explains 38.31% of the variation, in response, M=10 does 97.25\n",
    "# \n",
    "# \n",
    "# ## PCA on the training data:\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "pca2 = PCA()\n",
    "\n",
    "# Scale the data\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "\n",
    "plt.plot(np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "lowest MSE_CV when M=6 components are used\n",
    "\n",
    "\n",
    "# ### MSE_test CV\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "X_reduced_test = pca2.transform(scale(X_test))[:,:7]\n",
    "\n",
    "# Train regression model on training data \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:7], y_train)\n",
    "\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced_test)\n",
    "mean_squared_error(y_test, pred)\n",
    "\n",
    "\n",
    "# How does this MSE compare to The ridge and lasso? Is the final model easier to interpret (no variable selection)?\n",
    "\n",
    "# In[259]:\n",
    "\n",
    "\n",
    "regr_test = LinearRegression()\n",
    "regr_test.fit(X_reduced, y)\n",
    "regr_test.coef_\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6.7.2 Partial Least Squares\n",
    "\n",
    "Scikit-learn PLSRegression gives same results as the pls package in R when using method='oscorespls'. However, the standard method used is 'kernelpls', which we'll use here. Feel free to try out both.\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "n = len(X_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "for i in np.arange(1, 20):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = model_selection.cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(-score)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(np.arange(1, 20), np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1)\n",
    "\n",
    "\n",
    "# The lowest cross-validation error occurs when only M=2 partial least squares dimensions are used. We now evaluate the corresponding test set MSE:\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "\n",
    "mean_squared_error(y_test, pls.predict(scale(X_test)))\n",
    "\n",
    "\n",
    "# The test MSE is again comparable to the test MSE obtained using ridge regression, the lasso, and PCR.\n",
    "\n",
    "# ## Partial Least Squares\n",
    "# \n",
    "# \n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "n = len(X_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "for i in np.arange(1, 20):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = model_selection.cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(-score)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(np.arange(1, 20), np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1)\n",
    "\n",
    "\n",
    "# The lowest cross-validation error occurs when only M=2 partial least squares dimensions are used. We now evaluate the corresponding test set MSE:\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "\n",
    "mean_squared_error(y_test, pls.predict(scale(X_test)))\n",
    "\n",
    "\n",
    "# The test MSE is again comparable to the test MSE obtained using ridge regression, the lasso, and PCR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #6.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random normal\n",
    "x = np.random.normal(56, 6.4, 100)\n",
    "e = np.random.normal(0, 3.2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 0.3 + 0.2*x - 0.88*x**2 + 0.43*x**3 + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge X and Y as a dataframe\n",
    "X = pd.DataFrame(x, columns=list('X'))\n",
    "y = pd.DataFrame(Y, columns=list('y'))\n",
    "y = y.y\n",
    "\n",
    "#Use X, X^2, ..., X^10 as predictors\n",
    "X['X2'] = X['X']**2\n",
    "X['X3'] = X['X']**3\n",
    "X['X4'] = X['X']**4\n",
    "X['X5'] = X['X']**5\n",
    "X['X6'] = X['X']**6\n",
    "X['X7'] = X['X']**7\n",
    "X['X8'] = X['X']**8\n",
    "X['X9'] = X['X']**9\n",
    "X['X10'] = X['X']**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def regsubsets(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 models on 1 predictors in 0.03582501411437988 seconds.\n",
      "Processed 45 models on 2 predictors in 0.12461686134338379 seconds.\n",
      "Processed 120 models on 3 predictors in 0.27191781997680664 seconds.\n",
      "Processed 210 models on 4 predictors in 0.431851863861084 seconds.\n",
      "Processed 252 models on 5 predictors in 0.5514779090881348 seconds.\n",
      "Processed 210 models on 6 predictors in 0.44208383560180664 seconds.\n",
      "Processed 120 models on 7 predictors in 0.2571079730987549 seconds.\n",
      "Total elapsed time: 2.137752056121826 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = regsubsets(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n",
    "\n",
    "warnings.filterwarnings('ignore') #hide warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSS</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.221574e+07</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.813299e+02</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.506626e+02</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.506327e+02</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.004899e+02</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.004317e+02</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.015207e+02</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RSS                                              model\n",
       "1  1.221574e+07  <statsmodels.regression.linear_model.Regressio...\n",
       "2  7.813299e+02  <statsmodels.regression.linear_model.Regressio...\n",
       "3  7.506626e+02  <statsmodels.regression.linear_model.Regressio...\n",
       "4  7.506327e+02  <statsmodels.regression.linear_model.Regressio...\n",
       "5  7.004899e+02  <statsmodels.regression.linear_model.Regressio...\n",
       "6  7.004317e+02  <statsmodels.regression.linear_model.Regressio...\n",
       "7  7.015207e+02  <statsmodels.regression.linear_model.Regressio..."
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Selection\n",
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  10 models on 1 predictors in 0.037576913833618164 seconds.\n",
      "Processed  9 models on 2 predictors in 0.050366878509521484 seconds.\n",
      "Processed  8 models on 3 predictors in 0.024449586868286133 seconds.\n",
      "Processed  7 models on 4 predictors in 0.020205020904541016 seconds.\n",
      "Processed  6 models on 5 predictors in 0.014930009841918945 seconds.\n",
      "Processed  5 models on 6 predictors in 0.016577959060668945 seconds.\n",
      "Processed  4 models on 7 predictors in 0.013983011245727539 seconds.\n",
      "Processed  3 models on 8 predictors in 0.008111715316772461 seconds.\n",
      "Processed  2 models on 9 predictors in 0.005714893341064453 seconds.\n",
      "Processed  1 models on 10 predictors in 0.0034058094024658203 seconds.\n",
      "Total elapsed time: 0.22117376327514648 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Selection\n",
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  10 models on 9 predictors in 0.03862309455871582 seconds.\n",
      "Processed  9 models on 8 predictors in 0.04897618293762207 seconds.\n",
      "Processed  8 models on 7 predictors in 0.030596017837524414 seconds.\n",
      "Processed  7 models on 6 predictors in 0.017754316329956055 seconds.\n",
      "Processed  6 models on 5 predictors in 0.015459060668945312 seconds.\n",
      "Processed  5 models on 4 predictors in 0.014005899429321289 seconds.\n",
      "Processed  4 models on 3 predictors in 0.010936975479125977 seconds.\n",
      "Processed  3 models on 2 predictors in 0.009359121322631836 seconds.\n",
      "Processed  2 models on 1 predictors in 0.004942178726196289 seconds.\n",
      "Total elapsed time: 0.199937105178833 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = X.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Best Subset:\n",
      "------------\n",
      "X    -3.344818e-01\n",
      "X2   -3.088202e+00\n",
      "X3    6.302071e-01\n",
      "X4   -7.117202e-03\n",
      "X5    1.249109e-04\n",
      "X6   -1.084410e-06\n",
      "X7    3.729038e-09\n",
      "dtype: float64\n",
      "-----------------\n",
      "Foward Selection:\n",
      "-----------------\n",
      "X3    5.079484e-01\n",
      "X2   -1.946931e+00\n",
      "X7   -7.343843e-09\n",
      "X6    5.327992e-07\n",
      "X8    3.133630e-11\n",
      "X4   -1.738953e-03\n",
      "X    -1.846055e-01\n",
      "dtype: float64\n",
      "-------------------\n",
      "Backward Selection:\n",
      "-------------------\n",
      "X    -3.344818e-01\n",
      "X2   -3.088202e+00\n",
      "X3    6.302071e-01\n",
      "X4   -7.117202e-03\n",
      "X5    1.249109e-04\n",
      "X6   -1.084410e-06\n",
      "X7    3.729038e-09\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"------------\")\n",
    "print(\"Best Subset:\")\n",
    "print(\"------------\")\n",
    "print(models_best.loc[7, \"model\"].params)\n",
    "print(\"-----------------\")\n",
    "print(\"Foward Selection:\")\n",
    "print(\"-----------------\")\n",
    "print(models_fwd.loc[7, \"model\"].params)\n",
    "print(\"-------------------\")\n",
    "print(\"Backward Selection:\")\n",
    "print(\"-------------------\")\n",
    "print(models_bwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999923012075264"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso Regression\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005],cv=10).fit(X, y)\n",
    "\n",
    "#accuracy\n",
    "lcv.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X9    -6.539662e-14\n",
      "X10   -1.583842e-15\n",
      "X8     1.706686e-13\n",
      "X7     4.788735e-10\n",
      "X6     7.754429e-08\n",
      "X5     9.477569e-06\n",
      "X4     1.036865e-03\n",
      "X3     1.090054e-01\n",
      "X2     1.183984e+01\n",
      "X      1.677650e+02\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = X.columns)\n",
    "print(coef.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Coefficients in the Lasso Model')"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoCV(alphas=[1, 0.1, 0.001, 0.0005], copy_X=True, cv=10, eps=0.001,\n",
      "        fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=None,\n",
      "        normalize=False, positive=False, precompute='auto', random_state=None,\n",
      "        selection='cyclic', tol=0.0001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "#Change response Y\n",
    "Y = 0.3 + 0.88*x**7 + e\n",
    "y = pd.DataFrame(Y, columns=list('y'))\n",
    "y = y.y\n",
    "\n",
    "\n",
    "#Lasso regression\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005],cv=10).fit(X, y)\n",
    "\n",
    "print(model_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 models on 1 predictors in 0.03441429138183594 seconds.\n",
      "Processed 45 models on 2 predictors in 0.12182497978210449 seconds.\n",
      "Processed 120 models on 3 predictors in 0.27187609672546387 seconds.\n",
      "Processed 210 models on 4 predictors in 0.4809279441833496 seconds.\n",
      "Processed 252 models on 5 predictors in 0.5653982162475586 seconds.\n",
      "Processed 210 models on 6 predictors in 0.45142197608947754 seconds.\n",
      "Processed 120 models on 7 predictors in 0.26282382011413574 seconds.\n",
      "Total elapsed time: 2.2143051624298096 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSS</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>767.064707</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>761.243972</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>750.212961</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>749.879708</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>740.414794</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>736.277321</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>736.285923</td>\n",
       "      <td>&lt;statsmodels.regression.linear_model.Regressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RSS                                              model\n",
       "1  767.064707  <statsmodels.regression.linear_model.Regressio...\n",
       "2  761.243972  <statsmodels.regression.linear_model.Regressio...\n",
       "3  750.212961  <statsmodels.regression.linear_model.Regressio...\n",
       "4  749.879708  <statsmodels.regression.linear_model.Regressio...\n",
       "5  740.414794  <statsmodels.regression.linear_model.Regressio...\n",
       "6  736.277321  <statsmodels.regression.linear_model.Regressio...\n",
       "7  736.285923  <statsmodels.regression.linear_model.Regressio..."
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Subset Selection\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = regsubsets(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n",
    "\n",
    "warnings.filterwarnings('ignore') #hide warnings\n",
    "\n",
    "models_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #6.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "College = sm.datasets.get_rdataset(\"College\",\"ISLR\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = College.iloc[:,2:]\n",
    "y = College.Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Adrian College</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Agnes Scott College</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Alaska Pacific University</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accept  Enroll  Top10perc  Top25perc  \\\n",
       "Abilene Christian University    1232     721         23         52   \n",
       "Adelphi University              1924     512         16         29   \n",
       "Adrian College                  1097     336         22         50   \n",
       "Agnes Scott College              349     137         60         89   \n",
       "Alaska Pacific University        146      55         16         44   \n",
       "\n",
       "                              F.Undergrad  P.Undergrad  Outstate  Room.Board  \\\n",
       "Abilene Christian University         2885          537      7440        3300   \n",
       "Adelphi University                   2683         1227     12280        6450   \n",
       "Adrian College                       1036           99     11250        3750   \n",
       "Agnes Scott College                   510           63     12960        5450   \n",
       "Alaska Pacific University             249          869      7560        4120   \n",
       "\n",
       "                              Books  Personal  PhD  Terminal  S.F.Ratio  \\\n",
       "Abilene Christian University    450      2200   70        78       18.1   \n",
       "Adelphi University              750      1500   29        30       12.2   \n",
       "Adrian College                  400      1165   53        66       12.9   \n",
       "Agnes Scott College             450       875   92        97        7.7   \n",
       "Alaska Pacific University       800      1500   76        72       11.9   \n",
       "\n",
       "                              perc.alumni  Expend  Grad.Rate  \n",
       "Abilene Christian University           12    7041         60  \n",
       "Adelphi University                     16   10527         56  \n",
       "Adrian College                         30    8735         54  \n",
       "Agnes Scott College                    37   19016         59  \n",
       "Alaska Pacific University               2   10922         15  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test,  y_train,  y_test  =  train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000457292126521"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#accuracy\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize X\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9032328113611048"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ridge regression\n",
    "rcv = RidgeCV(alphas=np.linspace(.01, 100, 1000),  cv=10)\n",
    "\n",
    "rcv.fit(X_train_scaled, y_train)\n",
    "\n",
    "#accuracy\n",
    "rcv.score(X_test_scaled,y_test)\n",
    "warnings.filterwarnings('ignore') #hide warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000490350110231"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso regression\n",
    "lcv = LassoCV(alphas=np.linspace(.01, 100, 1000),  cv=10)\n",
    "\n",
    "lcv.fit(X_train_scaled, y_train) \n",
    "\n",
    "#accuracy\n",
    "lcv.score(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44792060717782123"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Principal Component Regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "# Run PCA producing the reduced variable Xreg and select the first pc components\n",
    "X_reduced_train = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "X_reduced_test = pca.fit_transform(X_test_scaled)\n",
    "\n",
    "lr.fit(X_reduced_train, y_train)\n",
    "\n",
    "#accuracy\n",
    "lr.score(X_reduced_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.907573557280662"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Partial Least Squares Regression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "pls = PLSRegression(n_components=5)\n",
    "\n",
    "pls.fit(X_train_scaled, y_train)\n",
    "\n",
    "#accuracy\n",
    "pls.score(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy are not much different but PCR. The PLR seems to perform better than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #6.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data set\n",
    "np.random.seed(0)\n",
    "\n",
    "# Dataframe with random numbers and the specified dimensions\n",
    "n = 1000\n",
    "p = 20\n",
    "X = pd.DataFrame(np.random.normal(size=(n, p)))\n",
    "\n",
    "# Epsilon\n",
    "e = np.random.normal(size=n)\n",
    "\n",
    "# Coefficient b1\n",
    "b1 = np.random.normal(size=p)\n",
    "# Random number of b1 elements with value zero\n",
    "for i in range(0, np.random.randint(0,p)):\n",
    "    b1[np.random.randint(0,p)] = 0\n",
    "\n",
    "# model\n",
    "y = np.dot(X, b1) + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Subset Selection\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def getRSS_validation(y_train, X_train, y_test, X_test,  feature_list):\n",
    "    model = sm.OLS(y_train, X_train[list(feature_list)]).fit()\n",
    "    RSS = ((model.predict(X_test[list(feature_list)]) - y_test) ** 2).sum()\n",
    "    return {'Model':model, \"RSS\":RSS}\n",
    "\n",
    "def bestModel_validation(y_train, X_train, y_test, X_test, K):\n",
    "    results = []\n",
    "    for c in itertools.combinations(X_train.columns, K):\n",
    "        results.append(getRSS_validation(y_train, X_train, y_test, X_test, c))     \n",
    "    model_all =  pd.DataFrame(results)\n",
    "    \n",
    "    best_model = model_all.loc[model_all[\"RSS\"].argmin()] ## this could be modified to have the top several models\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature = 3\n",
    "\n",
    "models_validation = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "for i in range(1,(max_feature+1)):  # for illustration purpuse, I just run for 1 - max_fearure features \n",
    "    models_validation.loc[i] = bestModel_validation(y_train, X_train, y_test, X_test, i) \n",
    "    \n",
    "warnings.filterwarnings('ignore') #hide warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(models_validation[\"RSS\"])\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_feature variable from best subset selection on traning and validation split\n",
      "1    -1.500438\n",
      "3     2.136651\n",
      "15    2.027191\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Best max_feature variable from best subset selection on traning and validation split')\n",
    "print (models_validation.loc[max_feature, 'Model'].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "np.random.seed(seed = 21)\n",
    "train_index = np.random.choice(k, size = len(y), replace = True)  # Randomly assign each observations into folds\n",
    "cv_errors = pd.DataFrame(columns=range(1,k+1), index=range(1,len(X.columns) + 1))\n",
    "\n",
    "models_cv = pd.DataFrame(columns=[\"RSS\", \"Model\"])\n",
    "for j in range(1,k+1):\n",
    "    feature_list = []\n",
    "    for i in range(1,len(X.columns)+1):\n",
    "        models_cv.loc[i] = forward_select_validation(y[train_index!= (j-1)], X[train_index != (j-1)], \n",
    "                                                     y[train_index == (j-1)],X[train_index == (j-1)], \n",
    "                                                     feature_list)\n",
    "        \n",
    "        cv_errors[j][i] = models_cv.loc[i][\"RSS\"]\n",
    "        feature_list = models_cv.loc[i][\"Model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.570\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.567\n",
      "Method:                 Least Squares   F-statistic:                              232.1\n",
      "Date:                Fri, 13 Mar 2020   Prob (F-statistic):                   9.63e-158\n",
      "Time:                        17:12:14   Log-Likelihood:                         -2253.3\n",
      "No. Observations:                 881   AIC:                                      4517.\n",
      "Df Residuals:                     876   BIC:                                      4541.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "15             1.9298      0.112     17.264      0.000       1.710       2.149\n",
      "1             -1.6219      0.108    -14.968      0.000      -1.835      -1.409\n",
      "8             -1.4265      0.103    -13.846      0.000      -1.629      -1.224\n",
      "3              1.7699      0.109     16.301      0.000       1.557       1.983\n",
      "6              1.4911      0.106     14.046      0.000       1.283       1.699\n",
      "==============================================================================\n",
      "Omnibus:                        0.063   Durbin-Watson:                   1.953\n",
      "Prob(Omnibus):                  0.969   Jarque-Bera (JB):                0.123\n",
      "Skew:                           0.009   Prob(JB):                        0.940\n",
      "Kurtosis:                       2.945   Cond. No.                         1.12\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(models_cv.loc[5, \"Model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_errors_mean = cv_errors.mean(axis = 1)\n",
    "plt.figure()\n",
    "plt.plot(cv_errors_mean)\n",
    "plt.xlabel('# features')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #6.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boston = sm.datasets.get_rdataset(\"Boston\",\"MASS\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Boston.crim\n",
    "X = Boston.iloc[:,1:]\n",
    "\n",
    "X_train,  X_test,  y_train,  y_test  =  train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Subset Selection \n",
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y_train,X_train[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X_test[list(feature_set)]) - y_test) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def regsubsets(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason13nn/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: \n",
      "The current behaviour of 'Series.argmin' is deprecated, use 'idxmin'\n",
      "instead.\n",
      "The behavior of 'argmin' will be corrected to return the positional\n",
      "minimum in the future. For now, use 'series.values.argmin' or\n",
      "'np.argmin(np.array(values))' to get the position of the minimum\n",
      "row.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 13 models on 1 predictors in 0.05172896385192871 seconds.\n",
      "Processed 78 models on 2 predictors in 0.20470213890075684 seconds.\n",
      "Processed 286 models on 3 predictors in 0.7506909370422363 seconds.\n",
      "Processed 715 models on 4 predictors in 2.198258876800537 seconds.\n",
      "Processed 1287 models on 5 predictors in 3.4549171924591064 seconds.\n",
      "Processed 1716 models on 6 predictors in 4.514233112335205 seconds.\n",
      "Processed 1716 models on 7 predictors in 5.106358051300049 seconds.\n",
      "Total elapsed time: 16.377593994140625 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = regsubsets(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                   crim   R-squared (uncentered):                   0.475\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.472\n",
      "Method:                 Least Squares   F-statistic:                              170.5\n",
      "Date:                Fri, 13 Mar 2020   Prob (F-statistic):                    1.81e-53\n",
      "Time:                        12:48:53   Log-Likelihood:                         -1245.3\n",
      "No. Observations:                 379   AIC:                                      2495.\n",
      "Df Residuals:                     377   BIC:                                      2502.\n",
      "Df Model:                           2                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "rad            0.5643      0.034     16.603      0.000       0.498       0.631\n",
      "black         -0.0053      0.001     -4.479      0.000      -0.008      -0.003\n",
      "==============================================================================\n",
      "Omnibus:                      538.751   Durbin-Watson:                   2.171\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            83590.433\n",
      "Skew:                           7.173   Prob(JB):                         0.00\n",
      "Kurtosis:                      74.327   Cond. No.                         37.7\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Best model\n",
    "print(models_best.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize X for Lasso, Ridge, and PCR regression\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=array([1.00000000e-02, 1.10090090e-01, 2.10180180e-01, 3.10270270e-01,\n",
       "       4.10360360e-01, 5.10450450e-01, 6.10540541e-01, 7.10630631e-01,\n",
       "       8.10720721e-01, 9.10810811e-01, 1.01090090e+00, 1.11099099e+00,\n",
       "       1.21108108e+00, 1.31117117e+00, 1.41126126e+00, 1.51135135e+00,\n",
       "       1.61144144e+00, 1.71153153e+00, 1.81162162e+00, 1.91171171e+00,\n",
       "       2.01180180e+00, 2.11189189e+0...\n",
       "       9.88990090e+01, 9.89990991e+01, 9.90991892e+01, 9.91992793e+01,\n",
       "       9.92993694e+01, 9.93994595e+01, 9.94995495e+01, 9.95996396e+01,\n",
       "       9.96997297e+01, 9.97998198e+01, 9.98999099e+01, 1.00000000e+02]),\n",
       "        copy_X=True, cv=10, eps=0.001, fit_intercept=True, max_iter=1000,\n",
       "        n_alphas=100, n_jobs=None, normalize=False, positive=False,\n",
       "        precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "        verbose=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso regression\n",
    "lcv = LassoCV(alphas=np.linspace(.01, 100, 1000),  cv=10)\n",
    "\n",
    "lcv.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason13nn/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([1.00000000e-02, 1.10090090e-01, 2.10180180e-01, 3.10270270e-01,\n",
       "       4.10360360e-01, 5.10450450e-01, 6.10540541e-01, 7.10630631e-01,\n",
       "       8.10720721e-01, 9.10810811e-01, 1.01090090e+00, 1.11099099e+00,\n",
       "       1.21108108e+00, 1.31117117e+00, 1.41126126e+00, 1.51135135e+00,\n",
       "       1.61144144e+00, 1.71153153e+00, 1.81162162e+00, 1.91171171e+00,\n",
       "       2.01180180e+00, 2.11189189e+0...\n",
       "       9.80982883e+01, 9.81983784e+01, 9.82984685e+01, 9.83985586e+01,\n",
       "       9.84986486e+01, 9.85987387e+01, 9.86988288e+01, 9.87989189e+01,\n",
       "       9.88990090e+01, 9.89990991e+01, 9.90991892e+01, 9.91992793e+01,\n",
       "       9.92993694e+01, 9.93994595e+01, 9.94995495e+01, 9.95996396e+01,\n",
       "       9.96997297e+01, 9.97998198e+01, 9.98999099e+01, 1.00000000e+02]),\n",
       "        cv=10, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "        store_cv_values=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ridge regression\n",
    "rcv = RidgeCV(alphas=np.linspace(.01, 100, 1000),  cv=10)\n",
    "\n",
    "rcv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run PCA producing the reduced variable Xreg and select the first pc components\n",
    "X_reduced_train = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "X_reduced_test = pca.fit_transform(X_test_scaled)\n",
    "\n",
    "lr.fit(X_reduced_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4327626140028401"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy for Lasso\n",
    "lcv.score(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4255239919763707"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy for Ridge\n",
    "rcv.score(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably choose the lasso model because its test MSE is close to best and eliminates some predictors to reduce model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Not all the predictors add much value to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
